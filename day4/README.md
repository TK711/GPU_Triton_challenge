# Day 4: Fused AXPBY

## ğŸ“‹ ì‹¤ìŠµ ëª©í‘œ
`y = alpha * x + beta * y` - Fused ì—°ì‚° ë° Load/Store ìµœì í™”

---

## ğŸ“š ì‹¤ìŠµ ì „ ë³µìŠµ ì‚¬í•­

### 1. Day 1-3 ë³µìŠµ
- [ ] ê¸°ë³¸ ì»¤ë„ êµ¬ì¡° (program_id, offsets, mask)
- [ ] `tl.load`, `tl.store` ì‚¬ìš©ë²•
- [ ] Element-wise ì—°ì‚° íŒ¨í„´

### 2. Scalar Broadcasting (Day 2)
- [ ] Scalar ê°’ê³¼ í…ì„œì˜ ê³±ì…ˆ
- [ ] `alpha * x` íŒ¨í„´

### 3. í•µì‹¬ ê°œë…
```python
# ê¸°ë³¸ êµ¬ì¡°
@triton.jit
def kernel(x_ptr, y_ptr, alpha, beta, N, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < N
    
    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    # Fused ì—°ì‚°: alpha * x + beta * y
    result = alpha * x + beta * y
    tl.store(y_ptr + offsets, result, mask=mask)  # in-place
```

---

## ğŸ¯ ì‹¤ìŠµìœ¼ë¡œ ë°°ìš¸ ì‚¬í•­

### 1. Fused ì—°ì‚° ê°œë… â­ í•µì‹¬!
- [ ] ì—¬ëŸ¬ ì—°ì‚°ì„ í•˜ë‚˜ì˜ ì»¤ë„ë¡œ í†µí•©
- [ ] ë©”ëª¨ë¦¬ ì ‘ê·¼ ìµœì†Œí™”
- [ ] ì¤‘ê°„ ê²°ê³¼ë¥¼ SRAMì— ì €ì¥

### 2. Load/Store ìµœì í™”
- [ ] **Naive ë°©ì‹**: `y = alpha * x` â†’ `y = y + beta * y` (2ë²ˆì˜ ë©”ëª¨ë¦¬ ì ‘ê·¼)
- [ ] **Fused ë°©ì‹**: `y = alpha * x + beta * y` (1ë²ˆì˜ ë©”ëª¨ë¦¬ ì ‘ê·¼)
- [ ] ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ì ˆì•½

### 3. In-place ì—°ì‚°
- [ ] ì¶œë ¥ì„ ì…ë ¥ í…ì„œì— ì§ì ‘ ì €ì¥
- [ ] ì¶”ê°€ ë©”ëª¨ë¦¬ í• ë‹¹ ë¶ˆí•„ìš”

### 4. Scalar íŒŒë¼ë¯¸í„° ì „ë‹¬
```python
# Python ë˜í¼ì—ì„œ
def axpby_triton(x, y, alpha, beta):
    # alpha, betaëŠ” scalar ê°’
    kernel[grid](x, y, alpha, beta, N, BLOCK_SIZE=...)
```

---

## âš ï¸ ì£¼ì˜ ì‚¬í•­

### 1. ë©”ëª¨ë¦¬ ì ‘ê·¼ ìˆœì„œ
- **Load ìµœì í™”**: `x`ì™€ `y`ë¥¼ ë™ì‹œì— ë¡œë“œ
- **Store ìµœì í™”**: ê²°ê³¼ë¥¼ `y`ì— ì§ì ‘ ì €ì¥ (in-place)

### 2. In-place ì—°ì‚° ì£¼ì˜
- ì…ë ¥ `y`ê°€ ìˆ˜ì •ë¨!
- ì›ë³¸ ë³´ì¡´ì´ í•„ìš”í•˜ë©´ ë³µì‚¬ë³¸ ì‚¬ìš©
```python
# ì›ë³¸ ë³´ì¡´
y_copy = y.clone()
result = axpby_triton(x, y_copy, alpha, beta)
```

### 3. Scalar íƒ€ì…
- `alpha`, `beta`ëŠ” Python float ë˜ëŠ” torch scalar
- Tritonì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨

### 4. ìˆ˜ì¹˜ ì•ˆì •ì„±
- í° `alpha`, `beta` ê°’ ì£¼ì˜
- ì˜¤ë²„í”Œë¡œìš° ê°€ëŠ¥ì„± ì²´í¬

### 5. ë””ë²„ê¹… íŒ
```python
# ì‘ì€ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
N = 8
x = torch.ones(N, device='cuda') * 2.0
y = torch.ones(N, device='cuda') * 3.0
alpha, beta = 0.5, 0.7

# PyTorch ê²°ê³¼
expected = alpha * x + beta * y
result = axpby_triton(x, y, alpha, beta)

# yê°€ in-placeë¡œ ìˆ˜ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
assert torch.allclose(result, expected)
assert torch.allclose(y, expected)  # yë„ ìˆ˜ì •ë¨
```

---

## ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Day 1-3 ë³µìŠµ ì™„ë£Œ
- [ ] Fused ì—°ì‚° ê°œë… ì´í•´
- [ ] `y = alpha * x + beta * y` êµ¬í˜„
- [ ] In-place ì—°ì‚° ì´í•´
- [ ] Load/Store ìµœì í™” ì´í•´
- [ ] ë‹¤ì–‘í•œ alpha, beta ê°’ í…ŒìŠ¤íŠ¸
- [ ] PyTorch ê²°ê³¼ì™€ ë¹„êµ

---

## ğŸ”— ì°¸ê³  ìë£Œ

- Day 1-3 ì½”ë“œ
- Fused ì—°ì‚°ì€ Fused Softmax, Layer Norm, Fused Attentionì˜ ê¸°ì´ˆ

---

## ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„

ì´ Day 4ì—ì„œ ë°°ìš´ **Fused ì—°ì‚°** ê°œë…ì€:
- Fused Softmaxì—ì„œ í™œìš©
- Layer Normalizationì—ì„œ í™œìš©
- Fused Attentionì—ì„œ í•µì‹¬!